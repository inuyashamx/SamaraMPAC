#!/usr/bin/env python3
"""
Demo Interactivo del Sistema de Enrutamiento Inteligente
Permite al usuario probar diferentes tipos de consultas y ver cÃ³mo se enrutan
"""

from samara.smart_conversational_agent import SmartConversationalAgent
import time

def print_header(title):
    """Imprime un header bonito"""
    print("\n" + "="*70)
    print(f"ğŸ§  {title}")
    print("="*70)

def print_context_info(result):
    """Muestra informaciÃ³n del contexto y enrutamiento"""
    if result and isinstance(result, str) and "ğŸ¤–" in result:
        # Extraer informaciÃ³n del modelo del resultado
        lines = result.split('\n')
        model_line = [line for line in lines if line.startswith('ğŸ¤–')]
        if model_line:
            print(f"\nğŸ“Š {model_line[0]}")

def demo_interactivo():
    """Demo interactivo principal"""
    print_header("DEMO INTERACTIVO - ENRUTAMIENTO INTELIGENTE")
    
    print("ğŸ¯ Este demo te permite probar el sistema de enrutamiento basado en contexto")
    print("ğŸ’¡ Cada consulta serÃ¡ analizada y enrutada al modelo mÃ¡s apropiado")
    print("ğŸ“Š En modo dev verÃ¡s informaciÃ³n detallada del enrutamiento")
    
    try:
        # Inicializar agente en modo dev
        agente = SmartConversationalAgent(profile_path="profiles/dev.json")
        player_id = "demo_user"
        
        print(f"\nâœ… Sistema inicializado correctamente")
        print(f"ğŸ“‹ Proveedores disponibles: {agente.model_router.get_available_providers()}")
        
        # Ejemplos predefinidos
        ejemplos = [
            {
                "categoria": "Consulta Simple",
                "prompt": "Hola, Â¿cÃ³mo estÃ¡s?",
                "descripcion": "Saludo bÃ¡sico - deberÃ­a usar Ollama local"
            },
            {
                "categoria": "Pregunta TÃ©cnica",
                "prompt": "Â¿CuÃ¡l es la diferencia entre async/await y Promises en JavaScript?",
                "descripcion": "Pregunta tÃ©cnica mediana - Ollama puede manejarla"
            },
            {
                "categoria": "AnÃ¡lisis de CÃ³digo",
                "prompt": "Analiza este cÃ³digo React y sugiere mejoras:\n\n" + """
import React, { useState, useEffect } from 'react';

const UserProfile = ({ userId }) => {
  const [user, setUser] = useState(null);
  const [loading, setLoading] = useState(true);
  
  useEffect(() => {
    fetch(`/api/users/${userId}`)
      .then(response => response.json())
      .then(data => {
        setUser(data);
        setLoading(false);
      });
  }, [userId]);
  
  if (loading) return <div>Loading...</div>;
  
  return (
    <div>
      <h1>{user.name}</h1>
      <p>{user.email}</p>
    </div>
  );
};

export default UserProfile;
""" * 3,
                "descripcion": "AnÃ¡lisis de cÃ³digo mediano - contexto mÃ¡s grande"
            },
            {
                "categoria": "Comando de MigraciÃ³n",
                "prompt": "Migra el proyecto en C:\\MiProyecto de polymer a react",
                "descripcion": "Comando especializado - activarÃ¡ SamaraDevAgent"
            }
        ]
        
        while True:
            print_header("OPCIONES DISPONIBLES")
            print("1. ğŸ“ Escribir consulta personalizada")
            print("2. ğŸ§ª Probar ejemplos predefinidos")
            print("3. ğŸ“Š Ver estadÃ­sticas del sistema")
            print("4. ğŸ”§ Cambiar modelo forzado")
            print("5. âŒ Salir")
            
            opcion = input("\nğŸ¯ Elige una opciÃ³n (1-5): ").strip()
            
            if opcion == "1":
                # Consulta personalizada
                print("\nğŸ“ CONSULTA PERSONALIZADA")
                print("-" * 40)
                consulta = input("Escribe tu consulta: ").strip()
                
                if consulta:
                    print(f"\nğŸ”„ Procesando: {consulta[:50]}{'...' if len(consulta) > 50 else ''}")
                    
                    start_time = time.time()
                    respuesta = agente.interactuar(player_id, consulta)
                    end_time = time.time()
                    
                    print(f"\nğŸ’¬ RESPUESTA:")
                    print("-" * 40)
                    # Mostrar solo la respuesta principal (sin info del modelo)
                    respuesta_limpia = respuesta.split('\n\nğŸ¤–')[0] if '\n\nğŸ¤–' in respuesta else respuesta
                    print(respuesta_limpia)
                    
                    print_context_info(respuesta)
                    print(f"\nâ±ï¸ Tiempo de respuesta: {end_time - start_time:.2f} segundos")
            
            elif opcion == "2":
                # Ejemplos predefinidos
                print("\nğŸ§ª EJEMPLOS PREDEFINIDOS")
                print("-" * 40)
                
                for i, ejemplo in enumerate(ejemplos, 1):
                    print(f"{i}. {ejemplo['categoria']}")
                    print(f"   ğŸ“ {ejemplo['descripcion']}")
                
                ejemplo_num = input(f"\nElige un ejemplo (1-{len(ejemplos)}): ").strip()
                
                try:
                    idx = int(ejemplo_num) - 1
                    if 0 <= idx < len(ejemplos):
                        ejemplo = ejemplos[idx]
                        
                        print(f"\nğŸ§ª PROBANDO: {ejemplo['categoria']}")
                        print(f"ğŸ“ Prompt: {ejemplo['prompt'][:100]}{'...' if len(ejemplo['prompt']) > 100 else ''}")
                        print(f"ğŸ’¡ Expectativa: {ejemplo['descripcion']}")
                        
                        start_time = time.time()
                        respuesta = agente.interactuar(player_id, ejemplo['prompt'])
                        end_time = time.time()
                        
                        print(f"\nğŸ’¬ RESPUESTA:")
                        print("-" * 40)
                        respuesta_limpia = respuesta.split('\n\nğŸ¤–')[0] if '\n\nğŸ¤–' in respuesta else respuesta
                        print(respuesta_limpia[:300] + "..." if len(respuesta_limpia) > 300 else respuesta_limpia)
                        
                        print_context_info(respuesta)
                        print(f"\nâ±ï¸ Tiempo de respuesta: {end_time - start_time:.2f} segundos")
                    else:
                        print("âŒ NÃºmero de ejemplo invÃ¡lido")
                except ValueError:
                    print("âŒ Por favor ingresa un nÃºmero vÃ¡lido")
            
            elif opcion == "3":
                # EstadÃ­sticas
                print("\nğŸ“Š ESTADÃSTICAS DEL SISTEMA")
                print("-" * 40)
                stats = agente._get_comprehensive_stats()
                print(stats)
            
            elif opcion == "4":
                # Cambiar modelo
                print("\nğŸ”§ CONFIGURACIÃ“N DE MODELO")
                print("-" * 40)
                available = agente.model_router.get_available_providers()
                print(f"Proveedores disponibles: {', '.join(available)}")
                
                modelo = input("Forzar modelo (ollama/claude/gpt4/gemini) o 'auto' para automÃ¡tico: ").strip().lower()
                
                if modelo == "auto":
                    # Reinicializar para volver al modo automÃ¡tico
                    agente.model_router = agente.model_router.__class__()
                    print("âœ… Modo automÃ¡tico activado")
                elif modelo in ["ollama", "claude", "gpt4", "gemini"]:
                    respuesta = agente._handle_model_command(f"usar modelo {modelo}")
                    print(respuesta)
                else:
                    print("âŒ Modelo no vÃ¡lido")
            
            elif opcion == "5":
                print("\nğŸ‘‹ Â¡Gracias por probar el sistema de enrutamiento inteligente!")
                print("ğŸ’¡ Para usar Samara en producciÃ³n: python samara_chat.py dev")
                break
            
            else:
                print("âŒ OpciÃ³n no vÃ¡lida. Por favor elige 1-5.")
            
            input("\nâ¸ï¸ Presiona Enter para continuar...")
    
    except KeyboardInterrupt:
        print("\n\nâ¹ï¸ Demo interrumpido por el usuario")
    except Exception as e:
        print(f"\nâŒ Error en el demo: {e}")
        print("ğŸ’¡ Verifica que Ollama estÃ© corriendo o que tengas API keys configuradas")

def mostrar_info_inicial():
    """Muestra informaciÃ³n inicial del sistema"""
    print("ğŸš€ SAMARA - META-AGENTE ORQUESTADOR")
    print("Sistema de Enrutamiento Inteligente Basado en Contexto")
    print()
    print("ğŸ¯ CARACTERÃSTICAS:")
    print("   â€¢ AnÃ¡lisis automÃ¡tico del tamaÃ±o de contexto")
    print("   â€¢ SelecciÃ³n inteligente del modelo Ã³ptimo")
    print("   â€¢ OptimizaciÃ³n de costos (local vs cloud)")
    print("   â€¢ Fallback automÃ¡tico entre proveedores")
    print("   â€¢ EstadÃ­sticas detalladas de uso")
    print()
    print("ğŸ“Š ESTRATEGIA DE ENRUTAMIENTO:")
    print("   â€¢ Contexto pequeÃ±o (<2K tokens) â†’ Ollama (local, gratis)")
    print("   â€¢ Contexto mediano (2K-10K) â†’ SegÃºn tarea y disponibilidad")
    print("   â€¢ Contexto grande (10K-30K) â†’ Modelos cloud especializados")
    print("   â€¢ Contexto muy grande (>30K) â†’ Claude/GPT-4 (mÃ¡xima capacidad)")

if __name__ == "__main__":
    mostrar_info_inicial()
    demo_interactivo() 