# ğŸ§  Meta-Agente Orquestador de Samara

## ğŸ¯ DescripciÃ³n

El **Meta-Agente Orquestador** es el cerebro inteligente de Samara que decide dinÃ¡micamente quÃ© modelo de IA usar segÃºn la tarea, complejidad, costo y disponibilidad. Es un sistema completo que orquesta mÃºltiples LLMs y agentes especializados para ofrecer la mejor experiencia posible.

## ğŸš€ CaracterÃ­sticas Principales

### ğŸ¤– Enrutamiento Inteligente de Modelos
- **DetecciÃ³n automÃ¡tica** del tipo de tarea
- **AnÃ¡lisis de contexto** - considera el tamaÃ±o en tokens
- **SelecciÃ³n dinÃ¡mica** del mejor modelo para cada situaciÃ³n
- **Sistema de fallback** robusto con mÃºltiples proveedores
- **OptimizaciÃ³n de costos** y velocidad basada en contexto

### ğŸ§  Agentes Especializados
- **ContextAgent**: Decide cuÃ¡ndo usar memoria
- **MemoryAgent**: Gestiona quÃ© guardar en memoria
- **SamaraDevAgent**: Comandos especializados de desarrollo
- **ConversationManager**: Historial completo en Weaviate

### ğŸ“Š EstadÃ­sticas Avanzadas
- Uso por proveedor y tipo de tarea
- Ratios de eficiencia de memoria
- Fallbacks y errores
- Analytics por usuario

## ğŸ› ï¸ Proveedores Soportados

| Proveedor | Modelos | Contexto | Costo | Velocidad | Calidad | Uso Recomendado |
|-----------|---------|----------|-------|-----------|---------|-----------------|
| **Ollama** | Llama3, Mistral, Phi3 | 8K tokens | Gratis | RÃ¡pido | Bueno | ConversaciÃ³n, tareas simples |
| **Claude** | Claude-3 Opus/Sonnet | 200K tokens | Alto | Medio | Excelente | MigraciÃ³n compleja, contextos grandes |
| **GPT-4** | GPT-4, GPT-4 Turbo | 128K tokens | Medio-Alto | Medio | Excelente | Debugging, arquitectura, contextos grandes |
| **Gemini** | Gemini Pro | 32K tokens | Bajo-Medio | RÃ¡pido | Muy Bueno | DocumentaciÃ³n, contextos medianos |
| **Perplexity** | Llama-3 Sonar | 32K tokens | Bajo | RÃ¡pido | Bueno | BÃºsquedas, informaciÃ³n actual |

## ğŸ“‹ Tipos de Tareas Detectadas

### ğŸ”„ MigraciÃ³n
- **Compleja**: Proyectos completos (300k+ lÃ­neas) â†’ Claude/GPT-4
- **Sencilla**: Archivos individuales â†’ Ollama/Gemini

### ğŸ” AnÃ¡lisis y Desarrollo
- **AnÃ¡lisis de CÃ³digo**: Estructura, dependencias â†’ Claude/GPT-4
- **Debugging**: Errores, problemas â†’ GPT-4/Claude
- **Refactoring**: Mejoras de cÃ³digo â†’ Claude/Ollama
- **Arquitectura**: DiseÃ±o de sistemas â†’ Claude/GPT-4

### ğŸ“š DocumentaciÃ³n y Consultas
- **DocumentaciÃ³n**: Explicaciones, tutoriales â†’ Gemini/Ollama
- **Consulta Simple**: Preguntas generales â†’ Ollama/Perplexity
- **ConversaciÃ³n/Juego**: Modo game â†’ Ollama/Gemini

## âš™ï¸ ConfiguraciÃ³n

### 1. Variables de Entorno

Copia `env_example.txt` a `.env` y configura tus API keys:

```bash
# Claude (Anthropic)
CLAUDE_API_KEY=tu_claude_api_key_aqui

# OpenAI (GPT-4)
OPENAI_API_KEY=tu_openai_api_key_aqui

# Google Gemini
GEMINI_API_KEY=tu_gemini_api_key_aqui

# Perplexity AI
PERPLEXITY_API_KEY=tu_perplexity_api_key_aqui

# Ollama (local)
OLLAMA_URL=http://localhost:11434
```

### 2. Instalar Ollama (Recomendado)

```bash
# Instalar Ollama
curl -fsSL https://ollama.ai/install.sh | sh

# Descargar modelos
ollama pull llama3:instruct
ollama pull mistral
ollama pull phi3
```

### 3. Dependencias Python

```bash
pip install requests weaviate-client python-dotenv
```

## ğŸ® Uso

### Verificar ConfiguraciÃ³n (Recomendado primero)
```bash
python verificar_configuracion.py
```

### Modo BÃ¡sico
```bash
python samara_chat.py dev
```

### Demo Completo
```bash
python demo_meta_agente.py
```

### Demo de Enrutamiento Inteligente
```bash
python demo_contexto_inteligente.py
```

### Comandos Especiales

#### ğŸ”§ ConfiguraciÃ³n de Modelo
```
usar modelo ollama    # Fuerza Ollama para toda la sesiÃ³n
usar modelo claude    # Fuerza Claude para toda la sesiÃ³n
usar modelo gpt       # Fuerza GPT-4 para toda la sesiÃ³n
```

#### ğŸ“Š EstadÃ­sticas
```
stats                 # Muestra estadÃ­sticas completas
estadÃ­sticas         # Alias en espaÃ±ol
```

#### ğŸš€ Comandos de Desarrollo
```
migra el proyecto en /ruta de polymer a react
analiza el proyecto en /ruta
```

## ğŸ§  LÃ³gica de Enrutamiento

### DetecciÃ³n AutomÃ¡tica
El sistema analiza el prompt y detecta automÃ¡ticamente:

1. **Palabras clave especÃ­ficas**
2. **TamaÃ±o del contexto** (estimaciÃ³n en tokens)
3. **Longitud y complejidad**
4. **Modo de operaciÃ³n** (dev/game)
5. **Contexto de conversaciÃ³n**

### ğŸ“ Enrutamiento Basado en Contexto

#### EstimaciÃ³n de Tokens
- **Regla base**: ~4 caracteres = 1 token en espaÃ±ol
- **Ajuste por cÃ³digo**: +20% para contenido tÃ©cnico
- **Proyectos grandes**: MÃ­nimo 50k tokens estimados

#### Estrategia de SelecciÃ³n por TamaÃ±o

| TamaÃ±o de Contexto | Estrategia | Modelos Preferidos |
|-------------------|------------|-------------------|
| **Muy PequeÃ±o** (<1K tokens) | Local rÃ¡pido | Ollama |
| **PequeÃ±o** (1K-5K tokens) | Local si es simple | Ollama â†’ Gemini |
| **Mediano** (5K-20K tokens) | Modelo Ã³ptimo | SegÃºn tarea y capacidad |
| **Grande** (20K-50K tokens) | Cloud especializado | Gemini â†’ GPT-4 â†’ Claude |
| **Muy Grande** (>50K tokens) | MÃ¡xima capacidad | Claude â†’ GPT-4 |

### SelecciÃ³n de Proveedor
```python
# Ejemplo de lÃ³gica de decisiÃ³n mejorada
if contexto > 50000:  # Contexto muy grande
    preferir: Claude (200K) > GPT-4 (128K)
elif contexto > 20000:  # Contexto grande
    if tarea == MIGRACION_COMPLEJA:
        preferir: Claude > GPT-4 > Gemini
    elif tarea == DEBUGGING:
        preferir: GPT-4 > Claude > Gemini
elif contexto < 5000:  # Contexto pequeÃ±o
    preferir: Ollama (local, gratis)
else:  # Contexto mediano
    aplicar_logica_de_tarea()
```

### Sistema de Fallback
Si el proveedor principal falla:
1. Intenta con el siguiente en la lista de prioridad
2. Registra el fallback en estadÃ­sticas
3. Informa al usuario (solo en modo dev)

## ğŸ“ˆ Memoria Inteligente

### CuÃ¡ndo SE GUARDA en memoria:
- âœ… InformaciÃ³n tÃ©cnica importante
- âœ… Configuraciones de proyecto
- âœ… Errores y soluciones
- âœ… Preferencias del usuario

### CuÃ¡ndo NO se guarda:
- âŒ Saludos simples ("Hola", "Gracias")
- âŒ Confirmaciones bÃ¡sicas ("Ok", "SÃ­")
- âŒ Consultas muy generales
- âŒ InformaciÃ³n temporal

### CuÃ¡ndo SE USA memoria:
- ğŸ” Consultas tÃ©cnicas complejas
- ğŸ” Referencias a conversaciones previas
- ğŸ” Mensajes largos (>50 caracteres)
- ğŸ” Palabras clave tÃ©cnicas

## ğŸ“Š EstadÃ­sticas del Sistema

### MÃ©tricas de Eficiencia
- **Ratio de uso de memoria**: % de consultas que usan memoria
- **Ratio de guardado**: % de respuestas guardadas
- **Tasa de fallback**: % de consultas que usaron fallback
- **DistribuciÃ³n por proveedor**: Uso de cada modelo

### Ejemplo de Salida
```
ğŸ“Š EstadÃ­sticas Completas de Samara

ğŸ¯ Eficiencia del Sistema:
â€¢ Total de interacciones: 25
â€¢ Consultas a memoria: 8 (32.0%)
â€¢ Guardados en memoria: 5 (20.0%)
â€¢ Comandos de desarrollo: 3
â€¢ Fallbacks usados: 1

ğŸ¤– Uso de Modelos:
â€¢ Total de consultas: 25
â€¢ Fallbacks: 1
â€¢ Errores: 0

ğŸ“ˆ Por Proveedor:
â€¢ ollama: 15/20 (75.0% Ã©xito)
â€¢ claude: 3/3 (100.0% Ã©xito)
â€¢ gpt4: 2/2 (100.0% Ã©xito)
```

## ğŸ”§ Arquitectura del Sistema

```
SmartConversationalAgent
â”œâ”€â”€ ModelRouterAgent (Orquestador principal)
â”œâ”€â”€ ContextAgent (Decisiones de memoria)
â”œâ”€â”€ MemoryAgent (GestiÃ³n de recuerdos)
â”œâ”€â”€ ConversationManager (Historial completo)
â”œâ”€â”€ SamaraDevAgent (Comandos especializados)
â””â”€â”€ Proveedores:
    â”œâ”€â”€ Ollama (Local)
    â”œâ”€â”€ Claude (API)
    â”œâ”€â”€ GPT-4 (API)
    â”œâ”€â”€ Gemini (API)
    â””â”€â”€ Perplexity (API)
```

## ğŸ¯ Casos de Uso

### 1. Desarrollador Individual
- **MigraciÃ³n de proyectos**: Polymer â†’ React
- **Debugging**: AnÃ¡lisis de errores
- **Consultas rÃ¡pidas**: Ollama local
- **Tareas complejas**: Claude/GPT-4 automÃ¡tico

### 2. Equipo de Desarrollo
- **AnÃ¡lisis de arquitectura**: GPT-4 para diseÃ±o
- **DocumentaciÃ³n**: Gemini para explicaciones
- **Code reviews**: Claude para anÃ¡lisis profundo
- **Consultas diarias**: Ollama para velocidad

### 3. Empresa/Startup
- **MigraciÃ³n masiva**: 300k lÃ­neas con Claude
- **OptimizaciÃ³n de costos**: Ollama para tareas simples
- **Fallback garantizado**: MÃºltiples proveedores
- **Analytics detallados**: MÃ©tricas de uso

## ğŸš¨ Manejo de Errores

### Errores Comunes
1. **API Key no configurada**: Fallback automÃ¡tico a Ollama
2. **Ollama no disponible**: Mensaje de error claro
3. **Rate limiting**: Espera automÃ¡tica y reintento
4. **Timeout**: Fallback a proveedor mÃ¡s rÃ¡pido

### Logs y Debugging
- Errores registrados en estadÃ­sticas
- InformaciÃ³n de fallback en modo dev
- Timeouts configurables por proveedor

## ğŸ”® Roadmap Futuro

### PrÃ³ximas CaracterÃ­sticas
- [ ] **Auto-scaling**: Ajuste dinÃ¡mico segÃºn carga
- [ ] **Caching inteligente**: Respuestas similares
- [ ] **Fine-tuning**: Modelos personalizados
- [ ] **Multi-modal**: Soporte para imÃ¡genes/audio
- [ ] **Collaborative filtering**: Recomendaciones basadas en uso

### Optimizaciones Planeadas
- [ ] **PredicciÃ³n de costos**: EstimaciÃ³n antes de ejecutar
- [ ] **Load balancing**: DistribuciÃ³n inteligente de carga
- [ ] **A/B testing**: ComparaciÃ³n automÃ¡tica de modelos
- [ ] **Feedback loop**: Mejora continua basada en resultados

## ğŸ¤ Contribuir

1. Fork el repositorio
2. Crea una rama para tu feature
3. Implementa mejoras en el router o agentes
4. Agrega tests para nuevas funcionalidades
5. EnvÃ­a un pull request

## ğŸ“„ Licencia

MIT License - Ãšsalo libremente en tus proyectos.

---

**Â¡El Meta-Agente Orquestador de Samara estÃ¡ listo para revolucionar tu flujo de trabajo de desarrollo!** ğŸš€ 